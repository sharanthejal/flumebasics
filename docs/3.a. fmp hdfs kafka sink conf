# fmpkafka.conf: A single-node Flume configuration
# use the below configuration where the source is log and channels are file and memory for hdfs and kafka respectively

# Name the components on this agent
fmpkafka.sources = logsource
fmpkafka.sinks = kafkasink hdfssink
fmpkafka.channels = kafkachannel hdfschannel

# Describe/configure the source
fmpkafka.sources.logsource.type = exec
fmpkafka.sources.logsource.command = tail -f /home/ubuntu/sharan/flume/gen_logs/logs/access.log

# Describe the kafka sink
fmpkafka.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
#In the below line use the proper ip's where the kafka brokers are running
#fmpkafka.sinks.kafkasink.kafka.bootstrap.servers = ipaddress1:9092,ipaddress2:9092,ipaddress3:9092
fmpkafka.sinks.kafkasink.kafka.topic = myclustertkafkatopic

# Describe the HDFS sink
fmpkafka.sinks.hdfssink.type = hdfs
fmpkafka.sinks.hdfssink.hdfs.path = /user/flumetest
fmpkafka.sinks.hdfssink.hdfs.fileType = DataStream

# Use a channel which buffers events in memory kafkasink
fmpkafka.channels.kafkachannel.type = memory
fmp.channels.kafkachannel.capacity = 1000
fmp.channels.kafkachannel.transactionCapacity = 100

# Use a channel which buffers events in file for hdfssink
fmpkafka.channels.hdfschannel.type = file
fmpkafka.channels.hdfschannel.capacity = 1000
fmpkafka.channels.hdfschannel.transactionCapacity = 100

# Bind the source and sink to the channel
fmpkafka.sources.logsource.channels = kafkachannel hdfschannel
fmpkafka.sinks.kafkasink.channel = kafkachannel
fmpkafka.sinks.hdfssink.channel = hdfschannel

