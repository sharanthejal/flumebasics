--Below is the configuration file
# fmpkafka.conf: A single-node Flume configuration
# use the below configuration where the source is log and channels are file and memory for hdfs and kafka respectively

# Name the components on this agent
fmpkafka.sources = logsource
fmpkafka.sinks = kafkasink hdfssink
fmpkafka.channels = kafkachannel hdfschannel

# Describe/configure the source
fmpkafka.sources.logsource.type = exec
fmpkafka.sources.logsource.command = tail -f /home/ubuntu/sharan/flume/gen_logs/logs/access.log

# Describe the kafka sink
fmpkafka.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
#In the below line use the proper ip's where the kafka brokers are running
#fmpkafka.sinks.kafkasink.kafka.bootstrap.servers = ipaddress1:9092,ipaddress2:9092,ipaddress3:9092
fmpkafka.sinks.kafkasink.kafka.topic = myclustertkafkatopic

# Describe the HDFS sink
fmpkafka.sinks.hdfssink.type = hdfs
fmpkafka.sinks.hdfssink.hdfs.path = /user/flumetest
fmpkafka.sinks.hdfssink.hdfs.fileType = DataStream

# Use a channel which buffers events in memory kafkasink
fmpkafka.channels.kafkachannel.type = memory
fmp.channels.kafkachannel.capacity = 1000
fmp.channels.kafkachannel.transactionCapacity = 100

# Use a channel which buffers events in file for hdfssink
fmpkafka.channels.hdfschannel.type = file
fmpkafka.channels.hdfschannel.capacity = 1000
fmpkafka.channels.hdfschannel.transactionCapacity = 100

# Bind the source and sink to the channel
fmpkafka.sources.logsource.channels = kafkachannel hdfschannel
fmpkafka.sinks.kafkasink.channel = kafkachannel
fmpkafka.sinks.hdfssink.channel = hdfschannel

##Create a Kafka Topic
kafka-topics --zookeeper localhost:2181 --create --topic myclusterkafkatopic --partitions 2  --replication-factor 3
-- If we don't create a topic, topic will be created but it will take the default replication factor and partitions.
-- It's a good practice to create a topic and then push the data into that topic. 

## Start Flume Agent
--Start a Flume Agent with the above configuration file which will publish data to kafka topic and also push data to HDFS that's why
its a Multiplexing data flow model, that's why the naming also fmp(flume multiplexing, just for our understanding).

sudo -u hdfs flume-ng agent --name fmpkafka --conf-file /home/ubuntu/sharan/flume/fmpkafka.conf

## Now start the kafka consumer, which will consume the messages which are published to the topic from the beginnning
kafka-console-consumer \
--bootstrap-server ipaddress1:9092,ipaddress2:9092 \
--topic myclustertkafkatopic --from-beginning
